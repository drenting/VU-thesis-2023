{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"YxMOSwQ5lRcV"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["dir = '/content/drive/MyDrive/VU-thesis-2023/'"],"metadata":{"id":"bt2QHwW_lYSG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","!pip install transformers"],"metadata":{"id":"rnvFrXV3qcNa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch import nn\n","from torch.nn import CrossEntropyLoss\n","from torch.optim import Adam\n","import numpy as np\n","import pandas as pd\n","from transformers import BertTokenizer\n","from transformers import BertModel, BertConfig\n","from transformers.models.bert.modeling_bert import BertPreTrainedModel\n","from transformers.modeling_outputs import SequenceClassifierOutput\n","from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n","import random\n","from tqdm import tqdm, trange\n","from sklearn.metrics import classification_report\n","import os\n","import shutil"],"metadata":{"id":"7nuttf4m2UgF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define functions and classes"],"metadata":{"id":"BTw9R7yYIMPA"}},{"cell_type":"code","source":["# code from https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f\n","# code from https://towardsdatascience.com/how-to-use-datasets-and-dataloader-in-pytorch-for-custom-text-data-270eed7f7c00"],"metadata":{"id":"eoYgPJYB91I_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","class Dataset1(Dataset):\n","\n","    def __init__(self, df, taskname, label_map):\n","\n","        self.labels = [label_map[label] for label in df['labels']]\n","        self.texts = [tokenizer(text,\n","                               padding='max_length', max_length = 40, truncation=True,\n","                                return_tensors=\"pt\") for text in df['text']]\n","        self.taskname = taskname\n","\n","    def classes(self):\n","        return self.labels\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def get_batch_labels(self, idx):\n","        # Fetch a batch of labels\n","        return self.labels[idx]\n","\n","    def get_batch_texts(self, idx):\n","        # Fetch a batch of inputs\n","        return self.texts[idx]\n","\n","    def __getitem__(self, idx):\n","\n","        batch_texts = self.get_batch_texts(idx)\n","        batch_y = self.get_batch_labels(idx)\n","        taskname = self.taskname\n","\n","        return batch_texts, batch_y, taskname"],"metadata":{"id":"PKEG4GwR3OiU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class BertMultitaskClassifier(BertPreTrainedModel):\n","    def __init__(self, config, labels_map):\n","        super().__init__(config)\n","        self.num_labels1 = labels_map[0]\n","        self.num_labels2 = labels_map[1]\n","        self.num_labels3 = labels_map[2]\n","        self.config = config\n","\n","        self.bert = BertModel(config)\n","        classifier_dropout = (\n","            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n","        )\n","        self.dropout = nn.Dropout(classifier_dropout)\n","        self.classifier1 = nn.Linear(config.hidden_size, self.num_labels1)\n","        self.classifier2 = nn.Linear(config.hidden_size, self.num_labels2)\n","        self.classifier3 = nn.Linear(config.hidden_size, self.num_labels3)\n","\n","\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask = None,\n","        token_type_ids = None,\n","        position_ids = None,\n","        head_mask = None,\n","        inputs_embeds = None,\n","        labels = None,\n","        output_attentions = None,\n","        output_hidden_states = None,\n","        return_dict = None,\n","        taskname=None\n","    ):\n","        r\"\"\"\n","        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n","            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n","            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n","            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        pooled_output = outputs[1]\n","\n","        pooled_output = self.dropout(pooled_output)\n","        if taskname.item()==1:logits = self.classifier1(pooled_output) #taskname is a torch tensor\n","        if taskname.item()==2:logits = self.classifier2(pooled_output)\n","        if taskname.item()==3:logits = self.classifier3(pooled_output)\n","\n","\n","        loss = None\n","        if labels is not None:\n","            self.config.problem_type = 'single_label_classification'\n","\n","\n","            if self.config.problem_type == \"regression\":\n","                loss_fct = MSELoss()\n","                if self.num_labels[0] == 1:\n","                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n","                else:\n","                    loss = loss_fct(logits, labels)\n","            elif self.config.problem_type == \"single_label_classification\":\n","                loss_fct = CrossEntropyLoss()\n","                if taskname.item()==1:\n","                  loss = loss_fct(logits.view(-1, self.num_labels1), labels.view(-1))\n","                if taskname.item()==2:\n","                  loss = loss_fct(logits.view(-1, self.num_labels2), labels.view(-1))\n","                if taskname.item()==3:\n","                  loss = loss_fct(logits.view(-1, self.num_labels3), labels.view(-1))\n","\n","\n","            elif self.config.problem_type == \"multi_label_classification\":\n","                loss_fct = BCEWithLogitsLoss()\n","                loss = loss_fct(logits, labels)\n","        if not return_dict:\n","            output = (logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return SequenceClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )\n"],"metadata":{"id":"a_rRU4GQFwU-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def train(train_df1, train_df2, train_df3,\n","          labels_dict, num_labels, batchsize, num_epochs, learning_rate,\n","          eval_df=None, eval_task=None, eval_result_file=None, model_save_dir=None):\n","\n","\n","  print('----Preparing data----')\n","  labels_dict1 = labels_dict[1]\n","  labels_dict2 = labels_dict[2]\n","  labels_dict3 = labels_dict[3]\n","\n","\n","  train_data1 = Dataset1(train_df1, '1', labels_dict1)\n","  train_data2 = Dataset1(train_df2, '2', labels_dict2)\n","  train_data3 = Dataset1(train_df3, '3', labels_dict3)\n","\n","\n","  a=[]\n","  for i in range(int(len(train_data1)/batchsize)):\n","      a.append(1)\n","  for i in range(int(len(train_data2)/batchsize)):\n","      a.append(2)\n","  for i in range(int(len(train_data3)/batchsize)):\n","      a.append(3)\n","\n","\n","  print(\"len(a)=\",len(a), 'so there are', len(a), 'training batches per epoch.')\n","  random.shuffle(a)\n","  print('There are', a.count(1), 'batches for task one.')\n","  print('There are', a.count(2), 'batches for task two.')\n","  print('There are', a.count(3), 'batches for task three.')\n","\n","  use_cuda = torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","  model = BertMultitaskClassifier.from_pretrained(\"bert-base-uncased\", labels_map=num_labels)\n","\n","  optimizer = Adam(model.parameters(), lr= learning_rate)\n","  model.to(device)\n","\n","  epoch=0\n","  for _ in trange(num_epochs, desc=\"Epoch\"):\n","      print('----Training----')\n","      dataloader1 = DataLoader(train_data1, batch_size=batchsize, shuffle=True)\n","      dataloader2 = DataLoader(train_data2, batch_size=batchsize, shuffle=True)\n","      dataloader3 = DataLoader(train_data3, batch_size=batchsize, shuffle=True)\n","\n","      random.shuffle(a)\n","      print(\"\\na[:20]=\",a[:20])\n","      epoch+=1\n","      model.train()\n","      tr_loss = 0\n","      nb_tr_examples, nb_tr_steps = 0, 0\n","      for step, number in enumerate((tqdm(a, desc=\"Iteration\"))):\n","          if number==1:batch=dataloader1.__iter__().__next__()\n","          if number==2:batch=dataloader2.__iter__().__next__()\n","          if number==3:batch=dataloader3.__iter__().__next__()\n","\n","          texts, labels, tasknames = batch\n","          input_ids = texts['input_ids'].squeeze(1)\n","          token_type_ids = texts['token_type_ids'].squeeze(1)\n","          attention_mask = texts['attention_mask'].squeeze(1)\n","\n","          input_ids = input_ids.to(device)\n","          token_type_ids = token_type_ids.to(device)\n","          attention_mask = attention_mask.to(device)\n","          labels = labels.to(device)\n","\n","          taskname = tasknames[0] #per batch all the tasknames are the same, so just taking the first one to pass to the bertmodel\n","          if taskname=='1':task_name = torch.tensor([1], device='cuda')\n","          if taskname=='2':task_name = torch.tensor([2], device='cuda')\n","          if taskname=='3':task_name = torch.tensor([3], device='cuda')\n","\n","\n","          output = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, labels=labels, taskname=task_name)\n","          loss = output.loss\n","\n","          nb_tr_steps += 1\n","          model.zero_grad()\n","          loss.backward()\n","          optimizer.step()\n","\n","          tr_loss += loss.item()\n","\n","      print('\\nmean loss:', tr_loss/nb_tr_steps)\n","\n","      if eval_df is not None:\n","        print('----Evaluating----')\n","        if eval_task == '1':labels_diction = labels_dict1\n","        if eval_task == '2':labels_diction=labels_dict2\n","        if eval_task == '3':labels_diction=labels_dict\n","\n","        eval_data = Dataset1(eval_df, eval_task, labels_diction)\n","        eval_dataloader = DataLoader(eval_data, batch_size=batchsize, shuffle=False)\n","\n","        model.eval()\n","\n","        all_labels = []\n","        all_outputs = []\n","\n","        for texts, labels, tasknames in eval_dataloader:\n","          input_ids = texts['input_ids'].squeeze(1)\n","          token_type_ids = texts['token_type_ids'].squeeze(1)\n","          attention_mask = texts['attention_mask'].squeeze(1)\n","\n","          input_ids = input_ids.to(device)\n","          token_type_ids = token_type_ids.to(device)\n","          attention_mask = attention_mask.to(device)\n","          labels = labels.to(device)\n","          taskname = tasknames[0] #per batch all the tasknames are the same, so just taking the first one to pass to the bertmodel\n","          if taskname=='1':task_name = torch.tensor([1], device='cuda')\n","          if taskname=='2':task_name = torch.tensor([2], device='cuda')\n","          if taskname=='3':task_name = torch.tensor([3], device='cuda')\n","\n","          with torch.no_grad():\n","            output = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, labels=labels, taskname=task_name)\n","\n","          logits = output.logits.detach().cpu().numpy()\n","          labels = labels.to('cpu').numpy()\n","          outputs = np.argmax(logits, axis=1) #outputs = predictions\n","\n","        for output in outputs:\n","          all_outputs.append(output)\n","        for label in labels:\n","          all_labels.append(label)\n","\n","        report = classification_report(all_labels, all_outputs, output_dict=True)\n","        macroavg = report['macro avg']['f1-score']\n","        with open(dir+eval_result_file, 'a') as out:\n","          out.write(str(learning_rate)+'\\t'+str(batchsize)+'\\t'+str(epoch)+'\\t'+str(macroavg)+'\\n')\n","\n","  print('----Done training!----')\n","  # save model\n","  if model_save_dir:\n","    print('----Saving model----')\n","    torch.save(model.state_dict(), 'MT-bert-base-uncased.pt')\n","    if not os.path.exists(model_save_dir):\n","        os.mkdir(model_save_dir)\n","\n","    shutil.copy('MT-bert-base-uncased.pt', model_save_dir)\n","  return model"],"metadata":{"id":"8SwsCPHxkcAV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict(model, eval_df, batchsize, task, labels_dict, use_gold_labels=False):\n","  use_cuda = torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","  print('----Preparing data----')\n","  eval_data = Dataset1(eval_df, task, labels_dict)\n","  eval_dataloader = DataLoader(eval_data, batch_size=batchsize, shuffle=False)\n","\n","  eval_loss, eval_accuracy = 0, 0\n","  nb_eval_steps, nb_eval_examples = 0, 0\n","\n","  all_labels = []\n","  all_outputs = []\n","\n","  model.eval()\n","  print('----Predicting----')\n","  for texts, labels, tasknames in eval_dataloader:\n","    input_ids = texts['input_ids'].squeeze(1)\n","    token_type_ids = texts['token_type_ids'].squeeze(1)\n","    attention_mask = texts['attention_mask'].squeeze(1)\n","\n","    input_ids = input_ids.to(device)\n","    token_type_ids = token_type_ids.to(device)\n","    attention_mask = attention_mask.to(device)\n","    if use_gold_labels:\n","      labels = labels.to(device)\n","    taskname = tasknames[0] #per batch all the tasknames are the same, so just taking the first one to pass to the bertmodel\n","    if taskname=='1':task_name = torch.tensor([1], device='cuda')\n","    if taskname=='2':task_name = torch.tensor([2], device='cuda')\n","    if taskname=='3':task_name = torch.tensor([3], device='cuda')\n","\n","    with torch.no_grad():\n","            #tmp_eval_loss, logits = model(input_ids, token_type_ids, attention_mask, task_name, labels)\n","      if use_gold_labels:\n","        output = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, labels=labels, taskname=task_name)\n","      else:\n","        output = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, labels=None, taskname=task_name)\n","\n","    logits = output.logits.detach().cpu().numpy()\n","    if use_gold_labels:\n","      labels = labels.to('cpu').numpy()\n","    outputs = np.argmax(logits, axis=1) #outputs = predictions\n","\n","    tmp_eval_accuracy=np.sum(outputs == labels)\n","\n","    if use_gold_labels:\n","      eval_loss += output.loss.mean().item()\n","      eval_accuracy += tmp_eval_accuracy\n","\n","    nb_eval_examples += input_ids.size(0)\n","    nb_eval_steps += 1\n","\n","    for output in outputs:\n","      all_outputs.append(output)\n","    if use_gold_labels:\n","      for label in labels:\n","        all_labels.append(label)\n","\n","  print('----Done!----')\n","  if use_gold_labels:\n","    eval_loss = eval_loss / nb_eval_steps\n","    print('eval loss:', eval_loss, '\\n')\n","    eval_accuracy = eval_accuracy / nb_eval_examples\n","    print('eval accuracy:', eval_accuracy, '\\n')\n","\n","  if use_gold_labels:\n","    return all_labels, all_outputs\n","  else:\n","    return all_outputs\n"],"metadata":{"id":"AgAN029nlvU1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def report(gold_labels, outputs, report_file=None):\n","  report = classification_report(gold_labels, outputs, output_dict=True)\n","  print(report)\n","  if report_file is not None:\n","    df_report = pd.DataFrame(report).transpose()\n","    df_report.to_csv(report_file, sep='\\t')\n"],"metadata":{"id":"yoG9PjQkzt5k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#train\n","def train_and_predict(df1, df2, df3,\n","                      labels_dict, num_labels, batchsize, epochs, lr,\n","                      model_save_dir,\n","                      all_preds_file,\n","                      report_file,\n","                      reverse_dict, run):\n","\n","  finetuned_model = train(df1, df2, df3,\n","                        labels_dict, num_labels, batchsize, epochs, lr,\n","                        model_save_dir=model_save_dir+str(run))\n","\n","  #predict with each classifier on the three gold datasets\n","  golds, preds = predict(finetuned_model, eval_hate, 8, '1', labels_dict[1], use_gold_labels=True) #main task\n","  auxpreds = predict(finetuned_model, eval_hate, 8, '2', labels_dict[1]) #aux task 1\n","  auxpreds2 = predict(finetuned_model, eval_hate, 8, '3', labels_dict[1]) #auxtask 2\n","\n","  #save classification reports\n","  report(golds, preds, report_file=report_file+str(run)+'.tsv')\n","\n","  #save all predictions on gold datasets\n","  with open(all_preds_file+str(run)+'.tsv',\"w\") as f:\n","      f.write(\"Gold\\tmaintask\\tauxtask1\\tauxtask2\\n\")\n","      for gold, pred, auxpred, auxpred2 in zip(golds, preds, auxpreds, auxpreds2):\n","        f.write(reverse_dict[1][gold]+\"\\t\"+reverse_dict[1][pred]+\"\\t\"+reverse_dict[2][auxpred]+\"\\t\"+reverse_dict[3][auxpred2]+\"\\n\")\n","\n",""],"metadata":{"id":"ULMPYp8qIFlw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### AbuseEval + emotion + irony"],"metadata":{"id":"E2sD6Wa5iUWD"}},{"cell_type":"code","source":["print('----Loading train data----')\n","train_hate1 = pd.read_csv(f'{dir}data/abuseeval/train.tsv', sep='\\t')\n","train_hate2 = pd.read_csv(f'{dir}data/abuseeval/dev.tsv', sep='\\t')\n","train_hate = pd.concat([train_hate1, train_hate2])\n","train_emo = pd.read_csv(f'{dir}data/emotion/tec.tsv', sep='\\t')\n","train_irony = pd.read_csv(f'{dir}data/irony/train.tsv', sep='\\t')\n","print('----Loading test data----')\n","eval_hate = pd.read_csv(f'{dir}data/abuseeval/test.tsv', sep='\\t')"],"metadata":{"id":"XzQoJ6PaiXfk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_dict = {1: {0:0, 1:1, 2:2},\n","               2: {'anger': 0, 'disgust':1, 'fear': 2, 'joy': 3, 'sadness': 4, 'surprise': 5},\n","               3: {0:0, 1:1}}\n","\n","reverse_labels_dict ={1: {0:'not_abuse', 1:'explicit_abuse', 2:'implicit_abuse'},\n","                      2: {0: 'anger', 1: 'disgust', 2: 'fear', 3: 'joy', 4: 'sadness', 5: 'surprise'},\n","                      3: {0:'not_irony', 1:'irony'}}\n","\n","num_labels = [3, 6, 2]"],"metadata":{"id":"4F9brgF4jC44"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","train_and_predict(train_hate, train_emo, train_irony,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/abuseeval+emoir_3ep_16_2e-5_RUN',\n","                  dir+'results/abuseeval+emoir_RUN',\n","                  dir+'results/abuseeval+emoir_report',\n","                  reverse_labels_dict, 1)"],"metadata":{"id":"E2qqWFMGlV2Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","train_and_predict(train_hate, train_emo, train_irony,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/abuseeval+emoir_3ep_16_2e-5_RUN',\n","                  dir+'results/abuseeval+emoir_RUN',\n","                  dir+'results/abuseeval+emoir_report',\n","                  reverse_labels_dict, 2)"],"metadata":{"id":"AwWio7HQlV8o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","train_and_predict(train_hate, train_emo, train_irony,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/abuseeval+emoir_3ep_16_2e-5_RUN',\n","                  dir+'results/abuseeval+emoir_RUN',\n","                  dir+'results/abuseeval+emoir_report',\n","                  reverse_labels_dict, 3)"],"metadata":{"id":"OvDyMU7NlWCi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","train_and_predict(train_hate, train_emo, train_irony,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/abuseeval+emoir_3ep_16_2e-5_RUN',\n","                  dir+'results/abuseeval+emoir_RUN',\n","                  dir+'results/abuseeval+emoir_report',\n","                  reverse_labels_dict, 4)"],"metadata":{"id":"bzVK69aVlWHi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","train_and_predict(train_hate, train_emo, train_irony,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/abuseeval+emoir_3ep_16_2e-5_RUN',\n","                  dir+'results/abuseeval+emoir_RUN',\n","                  dir+'results/abuseeval+emoir_report',\n","                  reverse_labels_dict, 5)"],"metadata":{"id":"928auwh4lWMQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### IHC + sentiment + emotion"],"metadata":{"id":"-THhbjEam8aM"}},{"cell_type":"code","source":["train_hate = pd.read_csv(f'{dir}data/implicithate/train1.tsv', sep='\\t')\n","\n","eval_hate = pd.read_csv(f'{dir}data/implicithate/test1.tsv', sep='\\t')\n","\n","train_sent = pd.read_csv(f'{dir}data/sentiment/test2016.tsv', sep='\\t')\n","train_emo = pd.read_csv(f'{dir}data/emotion/tec.tsv', sep='\\t')"],"metadata":{"id":"lUuwaaBiiDOI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_dict = {1: {'not_hate':0, 'explicit_hate':1, 'implicit_hate':2},\n","               2: {'neutral': 0, 'positive':1, 'negative': 2},\n","               3: {'anger': 0, 'disgust':1, 'fear': 2, 'joy': 3, 'sadness': 4, 'surprise': 5}}\n","\n","reverse_labels_dict ={1: {0: 'not_hate', 1: 'explicit_hate', 2: 'implicit_hate'},\n","                      2: {0: 'neutral', 1: 'positive', 2: 'negative'},\n","                      3: {0: 'anger', 1: 'disgust', 2: 'fear', 3: 'joy', 4: 'sadness', 5: 'surprise'}}\n","\n","num_labels = [3, 3, 6]"],"metadata":{"id":"FAHXTUYfjK35"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","train_and_predict(train_hate, train_sent, train_emo,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/ihc+sentemo_3ep_16_2e-5_RUN',\n","                  dir+'results/ihc+sentemo_RUN',\n","                  dir+'results/ihc+sentemo_report',\n","                  reverse_labels_dict, 1)"],"metadata":{"id":"MIugUOwbFbAc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","train_and_predict(train_hate, train_sent, train_emo,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/ihc+sentemo_3ep_16_2e-5_RUN',\n","                  dir+'results/ihc+sentemo_RUN',\n","                  dir+'results/ihc+sentemo_report',\n","                  reverse_labels_dict, 2)"],"metadata":{"id":"g4EwEd2qFbGk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","train_and_predict(train_hate, train_sent, train_emo,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/ihc+sentemo_3ep_16_2e-5_RUN',\n","                  dir+'results/ihc+sentemo_RUN',\n","                  dir+'results/ihc+sentemo_report',\n","                  reverse_labels_dict, 3)"],"metadata":{"id":"tXOvxOy_FbLr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","train_and_predict(train_hate, train_sent, train_emo,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/ihc+sentemo_3ep_16_2e-5_RUN',\n","                  dir+'results/ihc+sentemo_RUN',\n","                  dir+'results/ihc+sentemo_report',\n","                  reverse_labels_dict, 4)"],"metadata":{"id":"eU2ZEi6dFbQM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","train_and_predict(train_hate, train_sent, train_emo,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/ihc+sentemo_3ep_16_2e-5_RUN',\n","                  dir+'results/ihc+sentemo_RUN',\n","                  dir+'results/ihc+sentemo_report',\n","                  reverse_labels_dict, 5)"],"metadata":{"id":"0jzz82O5Fbjt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### IHC + sentiment + sarcasm"],"metadata":{"id":"lzDpbBjKhWne"}},{"cell_type":"code","source":["train_hate = pd.read_csv(f'{dir}data/implicithate/train1.tsv', sep='\\t')\n","\n","eval_hate = pd.read_csv(f'{dir}data/implicithate/test1.tsv', sep='\\t')\n","\n","train_sent = pd.read_csv(f'{dir}data/sentiment/test2016.tsv', sep='\\t')\n","train_sarc1 = pd.read_csv(f'{dir}data/sarcasm/twitter-train-nocontext.tsv', sep='\\t')\n","train_sarc2 = pd.read_csv(f'{dir}data/sarcasm/reddit-train-nocontext.tsv', sep='\\t')\n","train_sarcasm = pd.concat([train_sarc1, train_sarc2])"],"metadata":{"id":"MHIE1zqghcrm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_dict = {1: {'not_hate':0, 'explicit_hate':1, 'implicit_hate':2},\n","               2: {'neutral': 0, 'positive':1, 'negative': 2},\n","               3: {'NOT_SARCASM': 0, 'SARCASM':1}}\n","\n","reverse_labels_dict ={1: {0: 'not_hate', 1: 'explicit_hate', 2: 'implicit_hate'},\n","                      2: {0: 'neutral', 1: 'positive', 2: 'negative'},\n","                      3: {0: 'not_sarcasm', 1: 'sarcasm'}}\n","\n","num_labels = [3, 3, 2]"],"metadata":{"id":"m3q_HhL5hcy1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","train_and_predict(train_hate, train_sent, train_sarcasm,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/ihc+sentsarc_3ep_16_2e-5_RUN',\n","                  dir+'results/ihc+sentsarc_RUN',\n","                  dir+'results/ihc+sentsarc_report',\n","                  reverse_labels_dict, 1)"],"metadata":{"id":"LzrMhjXRhzvV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","train_and_predict(train_hate, train_sent, train_sarcasm,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/ihc+sentsarc_3ep_16_2e-5_RUN',\n","                  dir+'results/ihc+sentsarc_RUN',\n","                  dir+'results/ihc+sentsarc_report',\n","                  reverse_labels_dict, 2)"],"metadata":{"id":"I_9MP0hThz1t"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","train_and_predict(train_hate, train_sent, train_sarcasm,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/ihc+sentsarc_3ep_16_2e-5_RUN',\n","                  dir+'results/ihc+sentsarc_RUN',\n","                  dir+'results/ihc+sentsarc_report',\n","                  reverse_labels_dict, 3)"],"metadata":{"id":"VE-lsG0qhz8c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","train_and_predict(train_hate, train_sent, train_sarcasm,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/ihc+sentsarc_3ep_16_2e-5_RUN',\n","                  dir+'results/ihc+sentsarc_RUN',\n","                  dir+'results/ihc+sentsarc_report',\n","                  reverse_labels_dict, 4)"],"metadata":{"id":"_RDA8b33h0Dj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","train_and_predict(train_hate, train_sent, train_sarcasm,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/ihc+sentsarc_3ep_16_2e-5_RUN',\n","                  dir+'results/ihc+sentsarc_RUN',\n","                  dir+'results/ihc+sentsarc_report',\n","                  reverse_labels_dict, 5)"],"metadata":{"id":"FvbRD2O2h0KH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### IHC + sentiment + irony"],"metadata":{"id":"yyPm60WliFxU"}},{"cell_type":"code","source":["train_hate = pd.read_csv(f'{dir}data/implicithate/train1.tsv', sep='\\t')\n","\n","eval_hate = pd.read_csv(f'{dir}data/implicithate/test1.tsv', sep='\\t')\n","\n","train_sent = pd.read_csv(f'{dir}data/sentiment/test2016.tsv', sep='\\t')\n","train_irony = pd.read_csv(f'{dir}data/irony/train.tsv', sep='\\t')"],"metadata":{"id":"1P_Yt1ugiJkk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_dict = {1: {'not_hate':0, 'explicit_hate':1, 'implicit_hate':2},\n","               2: {'neutral': 0, 'positive':1, 'negative': 2},\n","               3: {0:0, 1:1}}\n","\n","reverse_labels_dict ={1: {0: 'not_hate', 1: 'explicit_hate', 2: 'implicit_hate'},\n","                      2: {0: 'neutral', 1: 'positive', 2: 'negative'},\n","                      3: {0: 'not_irony', 1: 'irony'}}\n","\n","num_labels = [3, 3, 2]"],"metadata":{"id":"uRTUuR35iJrE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","train_and_predict(train_hate, train_sent, train_irony,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/ihc+sentiro_3ep_16_2e-5_RUN',\n","                  dir+'results/ihc+sentiro_RUN',\n","                  dir+'results/ihc+sentiro_report',\n","                  reverse_labels_dict, 1)"],"metadata":{"id":"hQRZVYg0iJxc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","train_and_predict(train_hate, train_sent, train_irony,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/ihc+sentiro_3ep_16_2e-5_RUN',\n","                  dir+'results/ihc+sentiro_RUN',\n","                  dir+'results/ihc+sentiro_report',\n","                  reverse_labels_dict, 2)"],"metadata":{"id":"0ki-zvrCiJ4b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","train_and_predict(train_hate, train_sent, train_irony,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/ihc+sentiro_3ep_16_2e-5_RUN',\n","                  dir+'results/ihc+sentiro_RUN',\n","                  dir+'results/ihc+sentiro_report',\n","                  reverse_labels_dict, 3)"],"metadata":{"id":"mkKxslh7iJ-D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","train_and_predict(train_hate, train_sent, train_irony,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/ihc+sentiro_3ep_16_2e-5_RUN',\n","                  dir+'results/ihc+sentiro_RUN',\n","                  dir+'results/ihc+sentiro_report',\n","                  reverse_labels_dict, 4)"],"metadata":{"id":"7VCRcYe4iKDz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","train_and_predict(train_hate, train_sent, train_irony,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/ihc+sentiro_3ep_16_2e-5_RUN',\n","                  dir+'results/ihc+sentiro_RUN',\n","                  dir+'results/ihc+sentiro_report',\n","                  reverse_labels_dict, 5)"],"metadata":{"id":"ZkYnCSM0iKJv"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### IHC + emotion + sarcasm"],"metadata":{"id":"VndBCMLzjhjN"}},{"cell_type":"code","source":["train_hate = pd.read_csv(f'{dir}data/implicithate/train1.tsv', sep='\\t')\n","train_emo = pd.read_csv(f'{dir}data/emotion/tec.tsv', sep='\\t')\n","train_sarc1 = pd.read_csv(f'{dir}data/sarcasm/twitter-train-nocontext.tsv', sep='\\t')\n","train_sarc2 = pd.read_csv(f'{dir}data/sarcasm/reddit-train-nocontext.tsv', sep='\\t')\n","train_sarcasm = pd.concat([train_sarc1, train_sarc2])\n","\n","eval_hate = pd.read_csv(f'{dir}data/implicithate/test1.tsv', sep='\\t')"],"metadata":{"id":"7qJwB1KXjuv1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_dict = {1: {'not_hate':0, 'explicit_hate':1, 'implicit_hate':2},\n","               2: {'anger': 0, 'disgust':1, 'fear': 2, 'joy': 3, 'sadness': 4, 'surprise': 5},\n","               3: {'NOT_SARCASM': 0, 'SARCASM':1}}\n","\n","reverse_labels_dict ={1: {0: 'not_hate', 1: 'explicit_hate', 2: 'implicit_hate'},\n","                      2: {0: 'anger', 1: 'disgust', 2: 'fear', 3: 'joy', 4: 'sadness', 5: 'surprise'},\n","                      3: {0: 'not_sarcasm', 1: 'sarcasm'}}\n","\n","num_labels = [3, 6, 2]"],"metadata":{"id":"5CfpnQZFjw_2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for run in [1, 2, 3, 4, 5]:\n","  train_and_predict(train_hate, train_emo, train_sarcasm,\n","                    labels_dict, num_labels, 16, 3, 2e-5,\n","                    dir+'models/ihc+emosarc_3ep_16_2e-5_RUN',\n","                    dir+'results/ihc+emosarc_RUN',\n","                    dir+'results/ihc+emosarc_report',\n","                    reverse_labels_dict, run)"],"metadata":{"id":"4NiLQWeCkqvP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### IHC + emotion + irony"],"metadata":{"id":"cpanGZVMjhrV"}},{"cell_type":"code","source":["train_hate = pd.read_csv(f'{dir}data/implicithate/train1.tsv', sep='\\t')\n","train_emo = pd.read_csv(f'{dir}data/emotion/tec.tsv', sep='\\t')\n","train_irony = pd.read_csv(f'{dir}data/irony/train.tsv', sep='\\t')\n","eval_hate = pd.read_csv(f'{dir}data/implicithate/test1.tsv', sep='\\t')"],"metadata":{"id":"SJDwn882jvVs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_dict = {1: {'not_hate':0, 'explicit_hate':1, 'implicit_hate':2},\n","               2: {'anger': 0, 'disgust':1, 'fear': 2, 'joy': 3, 'sadness': 4, 'surprise': 5},\n","               3: {0:0, 1:1}}\n","\n","reverse_labels_dict ={1: {0: 'not_hate', 1: 'explicit_hate', 2: 'implicit_hate'},\n","                      2: {0: 'anger', 1: 'disgust', 2: 'fear', 3: 'joy', 4: 'sadness', 5: 'surprise'},\n","                      3: {0: 'not_irony', 1: 'irony'}}\n","\n","num_labels = [3, 6, 2]"],"metadata":{"id":"mpLWZMH-jxrV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for run in [1, 2, 3, 4, 5]:\n","  train_and_predict(train_hate, train_emo, train_irony,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/ihc+emoiro_3ep_16_2e-5_RUN',\n","                  dir+'results/ihc+emoiro_RUN',\n","                  dir+'results/ihc+emoiro_report',\n","                  reverse_labels_dict, run)"],"metadata":{"id":"gBr4QUzKkxpr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### IHC + sarcasm + irony"],"metadata":{"id":"rji8pyrpjhx2"}},{"cell_type":"code","source":["train_hate = pd.read_csv(f'{dir}data/implicithate/train1.tsv', sep='\\t')\n","\n","eval_hate = pd.read_csv(f'{dir}data/implicithate/test1.tsv', sep='\\t')\n","\n","train_sarc1 = pd.read_csv(f'{dir}data/sarcasm/twitter-train-nocontext.tsv', sep='\\t')\n","train_sarc2 = pd.read_csv(f'{dir}data/sarcasm/reddit-train-nocontext.tsv', sep='\\t')\n","train_sarcasm = pd.concat([train_sarc1, train_sarc2])\n","train_irony = pd.read_csv(f'{dir}data/irony/train.tsv', sep='\\t')"],"metadata":{"id":"jP0lyg7ykMi0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_dict = {1: {'not_hate':0, 'explicit_hate':1, 'implicit_hate':2},\n","               2: {'NOT_SARCASM': 0, 'SARCASM':1},\n","               3: {0:0, 1:1}}\n","\n","reverse_labels_dict ={1: {0: 'not_hate', 1: 'explicit_hate', 2: 'implicit_hate'},\n","                      2: {0: 'not_sarcasm', 1: 'sarcasm'},\n","                      3: {0: 'not_irony', 1: 'irony'}}\n","\n","num_labels = [3, 2, 2]"],"metadata":{"id":"fKMvG_tJkcUE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for run in [1, 2, 3, 4, 5]:\n","  train_and_predict(train_hate, train_sarcasm, train_irony,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/ihc+sarciro_3ep_16_2e-5_RUN',\n","                  dir+'results/ihc+sarciro_RUN',\n","                  dir+'results/ihc+sarciro_report',\n","                  reverse_labels_dict, run)"],"metadata":{"id":"Q7wtAHUqkMvc"},"execution_count":null,"outputs":[]}]}