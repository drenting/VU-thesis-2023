{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"YxMOSwQ5lRcV"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["dir = '/content/drive/MyDrive/VU-thesis-2023/'"],"metadata":{"id":"bt2QHwW_lYSG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","!pip install transformers"],"metadata":{"id":"rnvFrXV3qcNa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch import nn\n","from torch.nn import CrossEntropyLoss\n","from torch.optim import Adam\n","import numpy as np\n","import pandas as pd\n","from transformers import BertTokenizer\n","from transformers import BertModel\n","from transformers.models.bert.modeling_bert import BertPreTrainedModel\n","from transformers.modeling_outputs import SequenceClassifierOutput\n","from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n","import random\n","from tqdm import tqdm, trange\n","from sklearn.metrics import classification_report\n","from datetime import date\n","import os\n","import shutil\n","from scipy.special import softmax\n","import re"],"metadata":{"id":"7nuttf4m2UgF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# code from https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f\n","# code from https://towardsdatascience.com/how-to-use-datasets-and-dataloader-in-pytorch-for-custom-text-data-270eed7f7c00\n"],"metadata":{"id":"eoYgPJYB91I_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","class Dataset1(Dataset):\n","\n","    def __init__(self, df, label_map):\n","\n","        self.labels = [label_map[label] for label in df['labels']]\n","        self.texts = [tokenizer(text,\n","                               padding='max_length', max_length = 40, truncation=True,\n","                                return_tensors=\"pt\") for text in df['text']]\n","\n","    def classes(self):\n","        return self.labels\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def get_batch_labels(self, idx):\n","        # Fetch a batch of labels\n","        return self.labels[idx]\n","\n","    def get_batch_texts(self, idx):\n","        # Fetch a batch of inputs\n","        return self.texts[idx]\n","\n","    def __getitem__(self, idx):\n","\n","        batch_texts = self.get_batch_texts(idx)\n","        batch_y = self.get_batch_labels(idx)\n","\n","        # batch = {\"text\": batch_texts, \"label\": batch_y, 'taskname': taskname}\n","        return batch_texts, batch_y"],"metadata":{"id":"PKEG4GwR3OiU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n"],"metadata":{"id":"M1vCuFFy3OlM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class BertSingletaskClassifier(BertPreTrainedModel):\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","        self.config = config\n","\n","        self.bert = BertModel(config)\n","        classifier_dropout = (\n","            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n","        )\n","        self.dropout = nn.Dropout(classifier_dropout)\n","        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n","\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask = None,\n","        token_type_ids = None,\n","        position_ids = None,\n","        head_mask = None,\n","        inputs_embeds = None,\n","        labels = None,\n","        output_attentions = None,\n","        output_hidden_states = None,\n","        return_dict = None,\n","    ):\n","        r\"\"\"\n","        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n","            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n","            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n","            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        pooled_output = outputs[1]\n","\n","        pooled_output = self.dropout(pooled_output)\n","        logits = self.classifier(pooled_output)\n","\n","        loss = None\n","        if labels is not None:\n","            if self.config.problem_type is None:\n","                if self.num_labels == 1:\n","                    self.config.problem_type = \"regression\"\n","                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n","                    self.config.problem_type = \"single_label_classification\"\n","                else:\n","                    self.config.problem_type = \"multi_label_classification\"\n","\n","            if self.config.problem_type == \"regression\":\n","                loss_fct = MSELoss()\n","                if self.num_labels == 1:\n","                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n","                else:\n","                    loss = loss_fct(logits, labels)\n","            elif self.config.problem_type == \"single_label_classification\":\n","                loss_fct = CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            elif self.config.problem_type == \"multi_label_classification\":\n","                loss_fct = BCEWithLogitsLoss()\n","                loss = loss_fct(logits, labels)\n","        if not return_dict:\n","            output = (logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return SequenceClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )\n"],"metadata":{"id":"BiPOnN1BlhUL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def train(train_df, num_labels, labels_dict, batchsize, num_epochs, learning_rate, out_dir):\n","\n","  #make dataset from data\n","  train_data_hate = Dataset1(train_df, labels_dict)\n","  a=[]\n","  for i in range(int(len(train_data_hate)/batchsize)):\n","      a.append(1)\n","\n","  print(\"len(a)=\",len(a), 'so there are', len(a), 'training batches per epoch.')\n","  random.shuffle(a)\n","  #print(\"a=\",a)\n","  print('There are', a.count(1), 'batches for this task.')\n","\n","\n","  use_cuda = torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","  model = BertSingletaskClassifier.from_pretrained('bert-base-uncased', num_labels=num_labels)\n","  optimizer = Adam(model.parameters(), lr= learning_rate) #parameters?\n","  model.to('cuda')\n","\n","  epoch=0\n","  for _ in trange(num_epochs, desc=\"Epoch\"):\n","      dataloader1 = DataLoader(train_data_hate, batch_size=batchsize, shuffle=True)\n","\n","      random.shuffle(a)\n","      print(\"\\na[:20]=\",a[:20])\n","      epoch+=1\n","      model.train()\n","      tr_loss = 0\n","      nb_tr_examples, nb_tr_steps = 0, 0\n","      for step, number in enumerate((tqdm(a, desc=\"Iteration\"))):\n","          if number==1:batch=dataloader1.__iter__().__next__()\n","\n","\n","          texts, labels = batch\n","          input_ids = texts['input_ids'].squeeze(1)\n","          token_type_ids = texts['token_type_ids'].squeeze(1)\n","          attention_mask = texts['attention_mask'].squeeze(1)\n","\n","          input_ids = input_ids.to(device)\n","          token_type_ids = token_type_ids.to(device)\n","          attention_mask = attention_mask.to(device)\n","          labels = labels.to(device)\n","\n","\n","\n","          train_output = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, labels=labels)\n","          loss = train_output.loss\n","          tr_loss += loss.item()\n","          nb_tr_steps += 1\n","\n","          model.zero_grad()\n","          loss.backward()\n","          optimizer.step()\n","\n","      print('\\nmean loss:', tr_loss/nb_tr_steps)\n","\n","\n","  # save model\n","  torch.save(model.state_dict(), 'ST-bert-base-uncased.pt')\n","\n","  if not os.path.exists(out_dir):\n","      os.mkdir(out_dir)\n","\n","  shutil.copy('ST-bert-base-uncased.pt', out_dir)\n","\n","  return model"],"metadata":{"id":"BlyTx6UQlmfb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def test(model, eval_df, labels_dict, batchsize, output_file, report_file=None):\n","  use_cuda = torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","  eval_data = Dataset1(eval_df, labels_dict)\n","  eval_dataloader = DataLoader(eval_data, batch_size=batchsize, shuffle=False)\n","\n","  eval_loss, eval_accuracy = 0, 0\n","  nb_eval_steps, nb_eval_examples = 0, 0\n","\n","  all_labels = []\n","  all_outputs = []\n","\n","  model.eval()\n","  with open(output_file,\"w\") as f:\n","    f.write(\"Text\\tPrediction\\tGold\\n\")\n","    for texts, labels in eval_dataloader:\n","          input_ids = texts['input_ids'].squeeze(1)\n","          token_type_ids = texts['token_type_ids'].squeeze(1)\n","          attention_mask = texts['attention_mask'].squeeze(1)\n","\n","          input_ids = input_ids.to(device)\n","          token_type_ids = token_type_ids.to(device)\n","          attention_mask = attention_mask.to(device)\n","          labels = labels.to(device)\n","\n","          with torch.no_grad():\n","            #tmp_eval_loss, logits = model(input_ids, token_type_ids, attention_mask, labels)\n","            eval_output = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, labels=labels)\n","\n","          logits = eval_output.logits.detach().cpu().numpy()\n","          labels = labels.to('cpu').numpy()\n","          input_ids = input_ids.to('cpu')\n","          outputs = np.argmax(logits, axis=1) #outputs = predictions\n","          for id, pred, gold in zip(input_ids, outputs, labels): #write predictions and gold labels to file\n","            f.write(' '.join(tokenizer.convert_ids_to_tokens(id))+\"\\t\"+str(pred)+\"\\t\"+str(gold)+\"\\n\")\n","\n","          tmp_eval_accuracy=np.sum(outputs == labels)\n","\n","          eval_loss += eval_output.loss.mean().item()\n","          eval_accuracy += tmp_eval_accuracy\n","\n","          nb_eval_examples += input_ids.size(0)\n","          nb_eval_steps += 1\n","\n","          for output in outputs:\n","            all_outputs.append(output)\n","          for label in labels:\n","            all_labels.append(label)\n","\n","  eval_loss = eval_loss / nb_eval_steps\n","  eval_accuracy = eval_accuracy / nb_eval_examples\n","  print('eval accuracy:', eval_accuracy)\n","  print('eval loss:', eval_loss, '\\n')\n","\n","  report = classification_report(all_labels, all_outputs, output_dict=True)\n","  print(report)\n","  if report_file is not None:\n","    df_report = pd.DataFrame(report).transpose()\n","    df_report.to_csv(report_file, sep='\\t')"],"metadata":{"id":"AgAN029nlvU1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"d11ygqGwiUa8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"TLJlfTyhiUdK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### AbuseEval"],"metadata":{"id":"fPA41491iX8k"}},{"cell_type":"code","source":["print('----Loading train data----')\n","train_hate1 = pd.read_csv(f'{dir}data/abuseeval/train.tsv', sep='\\t')\n","train_hate2 = pd.read_csv(f'{dir}data/abuseeval/dev.tsv', sep='\\t')\n","train_hate = pd.concat([train_hate1, train_hate2])\n","print('----Loading test data----')\n","eval_hate = pd.read_csv(f'{dir}data/abuseeval/test.tsv', sep='\\t')\n","\n","labels_dict = {0:0, 1:1, 2:2}\n","num_labels = 3\n","\n","for run in [1, 2, 3, 4, 5]:\n","  finetuned_model = train(train_hate, num_labels, labels_dict, 16, 3, 2e-5, dir+'models/baseline_abuse_3ep_16_2e-5_RUN'+str(run))\n","  test(finetuned_model, eval_hate, labels_dict, 8, dir+'results/baseline_abuse_RUN'+str(run)+'.tsv' )"],"metadata":{"id":"X8KsdyoWiUgN"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Implicit Hate Corpus"],"metadata":{"id":"TUIqU2fPkNOT"}},{"cell_type":"code","source":["print('----Loading train data----')\n","train_hate = pd.read_csv(f'{dir}data/implicithate/train1.tsv', sep='\\t')\n","\n","print('----Loading test data----')\n","eval_hate = pd.read_csv(f'{dir}data/implicithate/test1.tsv', sep='\\t')\n","\n","labels_dict = {'not_hate':0, 'explicit_hate':1, 'implicit_hate':2}\n","num_labels = 3\n","\n","for run in [1, 2, 3, 4, 5]:\n","  finetuned_model = train(train_hate, num_labels, labels_dict, 16, 3, 2e-5, dir+'models/baseline_imp3class_3ep_16_2e-5_RUN'+str(run))\n","  test(finetuned_model, eval_hate, labels_dict, 8, dir+'results/baseline_imp3class_RUN'+str(run)+'.tsv', dir+'results/baseline_imp3class_report'+str(run)+'.tsv')"],"metadata":{"id":"i7UTprtKyWeT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### TRAC"],"metadata":{"id":"dpeoOWMHkQHD"}},{"cell_type":"code","source":["print('----Loading train data----')\n","train_hate1 = pd.read_csv(f'{dir}data/trac/train.tsv', sep='\\t')\n","train_hate2 = pd.read_csv(f'{dir}data/trac/dev.tsv', sep='\\t')\n","train_hate = pd.concat([train_hate1, train_hate2])\n","print('----Loading test data----')\n","eval_hate = pd.read_csv(f'{dir}data/trac/fb_test.tsv', sep='\\t')\n","\n","labels_dict = {'NAG':0, 'CAG':1, 'OAG':2}\n","num_labels = 3\n","\n","for run in [1, 2, 3, 4, 5]:\n","  finetuned_model = train(train_hate, num_labels, labels_dict, 16, 3, 2e-5, dir+'models/baseline_trac_3ep_16_2e-5_RUN'+str(run))\n","  test(finetuned_model, eval_hate, labels_dict, 8, dir+'results/baseline_trac_RUN'+str(run)+'.tsv' )"],"metadata":{"id":"x6z554E6iUi0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LkHWCr85iUqj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Composite (abuseval, implicit hate and abuseeval)"],"metadata":{"id":"nAjKVR8wIVIz"}},{"cell_type":"markdown","source":["#### Combine data"],"metadata":{"id":"fJgEHS6BJtWV"}},{"cell_type":"code","source":["# labels_dict2 = {'NAG':0, 'CAG':1, 'OAG':2}\n","# labels_dict3 = {'not_hate':0, 'explicit_hate':1, 'implicit_hate':2}\n","\n","print('----Loading train data----')\n","#abuseeval\n","train_hate11 = pd.read_csv(f'{dir}data/abuseeval/train.tsv', sep='\\t')\n","train_hate12 = pd.read_csv(f'{dir}data/abuseeval/dev.tsv', sep='\\t')\n","train_hate1 = pd.concat([train_hate11, train_hate12])\n","\n","#trac\n","train_hate21 = pd.read_csv(f'{dir}data/trac/train.tsv', sep='\\t')\n","train_hate22 = pd.read_csv(f'{dir}data/trac/dev.tsv', sep='\\t')\n","train_hate2 = pd.concat([train_hate21, train_hate22])\n","train_hate2.replace({'NAG':0, 'OAG':1, 'CAG':2}, inplace=True)\n","train_hate2.drop(columns=['id'], inplace=True)\n","\n","#implicit hate\n","train_hate3 = pd.read_csv(f'{dir}data/implicithate/train1.tsv', sep='\\t')\n","train_hate3.replace({'not_hate':0, 'explicit_hate':1, 'implicit_hate':2}, inplace=True)\n","\n","#combine\n","train_hate = pd.concat([train_hate1, train_hate2, train_hate3])\n","\n","print('----Loading test data----')\n","#abuseeval\n","eval_hate1 = pd.read_csv(f'{dir}data/abuseeval/test.tsv', sep='\\t')\n","#trac\n","eval_hate2 = pd.read_csv(f'{dir}data/trac/fb_test.tsv', sep='\\t')\n","eval_hate2.replace({'NAG':0, 'OAG':1, 'CAG':2}, inplace=True)\n","eval_hate2.drop(columns=['Id'], inplace=True)\n","#implicit hate\n","eval_hate3 = pd.read_csv(f'{dir}data/implicithate/test1.tsv', sep='\\t')\n","eval_hate3.replace({'not_hate':0, 'explicit_hate':1, 'implicit_hate':2}, inplace=True)\n","\n","#combine\n","#eval_hate = pd.concat([eval_hate1, eval_hate2, eval_hate3])"],"metadata":{"id":"c8zOFezLIUj0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["assert len(train_hate) == 45424, \"length \"\n","assert (train_hate['labels']==0).sum() == 27408, 'labels'\n","assert (train_hate['labels']==1).sum() == 6313, 'labels'\n","assert (train_hate['labels']==2).sum() == 11703, 'labels'"],"metadata":{"id":"k1Kw4MUXMnw4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### train/test"],"metadata":{"id":"gzZKGndRJw20"}},{"cell_type":"code","source":["num_labels = 3\n","labels_dict = {0:0, 1:1, 2:2}\n","\n","for run in [1, 2, 3, 4, 5]:\n","  finetuned_model = train(train_hate, num_labels, labels_dict, 16, 3, 2e-5, dir+'models/baseline_all_3ep_16_2e-5_RUN'+str(run))\n","  test(finetuned_model, eval_hate1, labels_dict, 8, dir+'results/baseline_all_abuse_RUN'+str(run)+'.tsv', dir+'results/baseline_all_abuse_report'+str(run)+'.tsv')\n","  test(finetuned_model, eval_hate2, labels_dict, 8, dir+'results/baseline_all_trac_RUN'+str(run)+'.tsv', dir+'results/baseline_all_trac_report'+str(run)+'.tsv')\n","  test(finetuned_model, eval_hate3, labels_dict, 8, dir+'results/baseline_all_imp_RUN'+str(run)+'.tsv', dir+'results/baseline_all_imp_report'+str(run)+'.tsv')"],"metadata":{"id":"zLrbKP-PiUtF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"1NFCCIf8qZ1A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"nrlRRNzLqZ6Y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"LtYI81u5JzUN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7xEQHiIkJzXx"},"execution_count":null,"outputs":[]}]}