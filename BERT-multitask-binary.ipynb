{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"YxMOSwQ5lRcV"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","source":["dir = '/content/drive/MyDrive/VU-thesis-2023/'"],"metadata":{"id":"bt2QHwW_lYSG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%%capture\n","\n","!pip install transformers"],"metadata":{"id":"rnvFrXV3qcNa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","from torch.utils.data import Dataset\n","from torch.utils.data import DataLoader\n","from torch import nn\n","from torch.nn import CrossEntropyLoss\n","from torch.optim import Adam\n","import numpy as np\n","import pandas as pd\n","from transformers import BertTokenizer\n","from transformers import BertModel, BertConfig\n","from transformers.models.bert.modeling_bert import BertPreTrainedModel\n","from transformers.modeling_outputs import SequenceClassifierOutput\n","from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n","import random\n","from tqdm import tqdm, trange\n","from sklearn.metrics import classification_report\n","import os\n","import shutil"],"metadata":{"id":"7nuttf4m2UgF"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Define functions and classes"],"metadata":{"id":"BTw9R7yYIMPA"}},{"cell_type":"code","source":["# code from https://towardsdatascience.com/text-classification-with-bert-in-pytorch-887965e5820f\n","# code from https://towardsdatascience.com/how-to-use-datasets-and-dataloader-in-pytorch-for-custom-text-data-270eed7f7c00\n"],"metadata":{"id":"eoYgPJYB91I_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n","\n","class Dataset1(Dataset):\n","\n","    def __init__(self, df, taskname, label_map):\n","\n","        self.labels = [label_map[label] for label in df['labels']]\n","        self.texts = [tokenizer(text,\n","                               padding='max_length', max_length = 40, truncation=True,\n","                                return_tensors=\"pt\") for text in df['text']]\n","        self.taskname = taskname\n","\n","    def classes(self):\n","        return self.labels\n","\n","    def __len__(self):\n","        return len(self.labels)\n","\n","    def get_batch_labels(self, idx):\n","        # Fetch a batch of labels\n","        return self.labels[idx]\n","\n","    def get_batch_texts(self, idx):\n","        # Fetch a batch of inputs\n","        return self.texts[idx]\n","\n","    def __getitem__(self, idx):\n","\n","        batch_texts = self.get_batch_texts(idx)\n","        batch_y = self.get_batch_labels(idx)\n","        taskname = self.taskname\n","\n","        return batch_texts, batch_y, taskname"],"metadata":{"id":"PKEG4GwR3OiU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class BertMultitaskClassifier(BertPreTrainedModel):\n","    def __init__(self, config, labels_map):\n","        super().__init__(config)\n","        self.num_labels1 = labels_map[0]\n","        self.num_labels2 = labels_map[1]\n","\n","        self.config = config\n","\n","        self.bert = BertModel(config)\n","        classifier_dropout = (\n","            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n","        )\n","        self.dropout = nn.Dropout(classifier_dropout)\n","        self.classifier1 = nn.Linear(config.hidden_size, self.num_labels1)\n","        self.classifier2 = nn.Linear(config.hidden_size, self.num_labels2)\n","\n","\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask = None,\n","        token_type_ids = None,\n","        position_ids = None,\n","        head_mask = None,\n","        inputs_embeds = None,\n","        labels = None,\n","        output_attentions = None,\n","        output_hidden_states = None,\n","        return_dict = None,\n","        taskname=None\n","    ):\n","        r\"\"\"\n","        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n","            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n","            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n","            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        pooled_output = outputs[1]\n","\n","        pooled_output = self.dropout(pooled_output)\n","        if taskname.item()==1:logits = self.classifier1(pooled_output) #taskname is a torch tensor\n","        if taskname.item()==2:logits = self.classifier2(pooled_output)\n","\n","\n","\n","        loss = None\n","        if labels is not None:\n","            self.config.problem_type = 'single_label_classification'\n","\n","\n","            if self.config.problem_type == \"regression\":\n","                loss_fct = MSELoss()\n","                if self.num_labels[0] == 1:\n","                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n","                else:\n","                    loss = loss_fct(logits, labels)\n","            elif self.config.problem_type == \"single_label_classification\":\n","                loss_fct = CrossEntropyLoss()\n","                if taskname.item()==1:\n","                  loss = loss_fct(logits.view(-1, self.num_labels1), labels.view(-1))\n","                if taskname.item()==2:\n","                  loss = loss_fct(logits.view(-1, self.num_labels2), labels.view(-1))\n","\n","\n","\n","            elif self.config.problem_type == \"multi_label_classification\":\n","                loss_fct = BCEWithLogitsLoss()\n","                loss = loss_fct(logits, labels)\n","        if not return_dict:\n","            output = (logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return SequenceClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )\n"],"metadata":{"id":"a_rRU4GQFwU-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def train(train_df1, train_df2,\n","          labels_dict, num_labels, batchsize, num_epochs, learning_rate,\n","          eval_df=None, eval_task=None, eval_result_file=None, model_save_dir=None):\n","\n","\n","  print('----Preparing data----')\n","  labels_dict1 = labels_dict[1]\n","  labels_dict2 = labels_dict[2]\n","\n","\n","  train_data1 = Dataset1(train_df1, '1', labels_dict1)\n","  train_data2 = Dataset1(train_df2, '2', labels_dict2)\n","\n","\n","  a=[]\n","  for i in range(int(len(train_data1)/batchsize)):\n","      a.append(1)\n","  for i in range(int(len(train_data2)/batchsize)):\n","      a.append(2)\n","\n","\n","  print(\"len(a)=\",len(a), 'so there are', len(a), 'training batches per epoch.')\n","  random.shuffle(a)\n","  #print(\"a=\",a)\n","  print('There are', a.count(1), 'batches for task one.')\n","  print('There are', a.count(2), 'batches for task two.')\n","\n","\n","  use_cuda = torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","\n","  model = BertMultitaskClassifier.from_pretrained(\"bert-base-uncased\", labels_map=num_labels) #config=config\n","\n","  optimizer = Adam(model.parameters(), lr= learning_rate)\n","  model.to(device)\n","\n","  epoch=0\n","  for _ in trange(num_epochs, desc=\"Epoch\"):\n","      print('----Training----')\n","      dataloader1 = DataLoader(train_data1, batch_size=batchsize, shuffle=True)\n","      dataloader2 = DataLoader(train_data2, batch_size=batchsize, shuffle=True)\n","\n","      random.shuffle(a)\n","      print(\"\\na[:20]=\",a[:20])\n","      epoch+=1\n","      model.train()\n","      tr_loss = 0\n","      nb_tr_examples, nb_tr_steps = 0, 0\n","      for step, number in enumerate((tqdm(a, desc=\"Iteration\"))):\n","          if number==1:batch=dataloader1.__iter__().__next__()\n","          if number==2:batch=dataloader2.__iter__().__next__()\n","\n","          texts, labels, tasknames = batch\n","          input_ids = texts['input_ids'].squeeze(1)\n","          token_type_ids = texts['token_type_ids'].squeeze(1)\n","          attention_mask = texts['attention_mask'].squeeze(1)\n","\n","          input_ids = input_ids.to(device)\n","          token_type_ids = token_type_ids.to(device)\n","          attention_mask = attention_mask.to(device)\n","          labels = labels.to(device)\n","\n","          taskname = tasknames[0] #per batch all the tasknames are the same, so just taking the first one to pass to the bertmodel\n","          if taskname=='1':task_name = torch.tensor([1], device='cuda')\n","          if taskname=='2':task_name = torch.tensor([2], device='cuda')\n","\n","          output = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, labels=labels, taskname=task_name)\n","          loss = output.loss\n","\n","          nb_tr_steps += 1\n","          model.zero_grad()\n","          loss.backward()\n","          optimizer.step()\n","\n","          tr_loss += loss.item()\n","\n","      print('\\nmean loss:', tr_loss/nb_tr_steps)\n","\n","      if eval_df is not None:\n","        print('----Evaluating----')\n","        if eval_task == '1':labels_diction = labels_dict1\n","        if eval_task == '2':labels_diction=labels_dict2\n","\n","\n","        eval_data = Dataset1(eval_df, eval_task, labels_diction)\n","        eval_dataloader = DataLoader(eval_data, batch_size=batchsize, shuffle=False)\n","\n","        model.eval()\n","\n","        all_labels = []\n","        all_outputs = []\n","\n","        for texts, labels, tasknames in eval_dataloader:\n","          input_ids = texts['input_ids'].squeeze(1)\n","          token_type_ids = texts['token_type_ids'].squeeze(1)\n","          attention_mask = texts['attention_mask'].squeeze(1)\n","\n","          input_ids = input_ids.to(device)\n","          token_type_ids = token_type_ids.to(device)\n","          attention_mask = attention_mask.to(device)\n","          labels = labels.to(device)\n","          taskname = tasknames[0] #per batch all the tasknames are the same, so just taking the first one to pass to the bertmodel\n","          if taskname=='1':task_name = torch.tensor([1], device='cuda')\n","          if taskname=='2':task_name = torch.tensor([2], device='cuda')\n","\n","\n","          with torch.no_grad():\n","            output = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, labels=labels, taskname=task_name)\n","\n","          logits = output.logits.detach().cpu().numpy()\n","          labels = labels.to('cpu').numpy()\n","          outputs = np.argmax(logits, axis=1) #outputs = predictions\n","\n","        for output in outputs:\n","          all_outputs.append(output)\n","        for label in labels:\n","          all_labels.append(label)\n","\n","        report = classification_report(all_labels, all_outputs, output_dict=True)\n","        macroavg = report['macro avg']['f1-score']\n","        with open(dir+eval_result_file, 'a') as out:\n","          out.write(str(learning_rate)+'\\t'+str(batchsize)+'\\t'+str(epoch)+'\\t'+str(macroavg)+'\\n')\n","\n","  print('----Done training!----')\n","  # save model\n","  if model_save_dir:\n","    print('----Saving model----')\n","    torch.save(model.state_dict(), 'MT-bert-base-uncased.pt')\n","    if not os.path.exists(model_save_dir):\n","        os.mkdir(model_save_dir)\n","\n","    shutil.copy('MT-bert-base-uncased.pt', model_save_dir)\n","  return model"],"metadata":{"id":"8SwsCPHxkcAV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def predict(model, eval_df, batchsize, task, labels_dict, use_gold_labels=False):\n","  use_cuda = torch.cuda.is_available()\n","  device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n","\n","  print('----Preparing data----')\n","  eval_data = Dataset1(eval_df, task, labels_dict)\n","  eval_dataloader = DataLoader(eval_data, batch_size=batchsize, shuffle=False)\n","\n","  eval_loss, eval_accuracy = 0, 0\n","  nb_eval_steps, nb_eval_examples = 0, 0\n","\n","  all_labels = []\n","  all_outputs = []\n","\n","  model.eval()\n","  print('----Predicting----')\n","  for texts, labels, tasknames in eval_dataloader:\n","    input_ids = texts['input_ids'].squeeze(1)\n","    token_type_ids = texts['token_type_ids'].squeeze(1)\n","    attention_mask = texts['attention_mask'].squeeze(1)\n","\n","    input_ids = input_ids.to(device)\n","    token_type_ids = token_type_ids.to(device)\n","    attention_mask = attention_mask.to(device)\n","    if use_gold_labels:\n","      labels = labels.to(device)\n","    taskname = tasknames[0] #per batch all the tasknames are the same, so just taking the first one to pass to the bertmodel\n","    if taskname=='1':task_name = torch.tensor([1], device='cuda')\n","    if taskname=='2':task_name = torch.tensor([2], device='cuda')\n","\n","\n","    with torch.no_grad():\n","            #tmp_eval_loss, logits = model(input_ids, token_type_ids, attention_mask, task_name, labels)\n","      if use_gold_labels:\n","        output = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, labels=labels, taskname=task_name)\n","      else:\n","        output = model(input_ids, token_type_ids=token_type_ids, attention_mask=attention_mask, labels=None, taskname=task_name)\n","\n","    logits = output.logits.detach().cpu().numpy()\n","    if use_gold_labels:\n","      labels = labels.to('cpu').numpy()\n","    outputs = np.argmax(logits, axis=1) #outputs = predictions\n","\n","    tmp_eval_accuracy=np.sum(outputs == labels)\n","\n","    if use_gold_labels:\n","      eval_loss += output.loss.mean().item()\n","      eval_accuracy += tmp_eval_accuracy\n","\n","    nb_eval_examples += input_ids.size(0)\n","    nb_eval_steps += 1\n","\n","    for output in outputs:\n","      all_outputs.append(output)\n","    if use_gold_labels:\n","      for label in labels:\n","        all_labels.append(label)\n","\n","  print('----Done!----')\n","  if use_gold_labels:\n","    eval_loss = eval_loss / nb_eval_steps\n","    print('eval loss:', eval_loss, '\\n')\n","    eval_accuracy = eval_accuracy / nb_eval_examples\n","    print('eval accuracy:', eval_accuracy, '\\n')\n","\n","  if use_gold_labels:\n","    return all_labels, all_outputs\n","  else:\n","    return all_outputs\n"],"metadata":{"id":"AgAN029nlvU1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def report(gold_labels, outputs, report_file=None):\n","  report = classification_report(gold_labels, outputs, output_dict=True)\n","  print(report)\n","  if report_file is not None:\n","    df_report = pd.DataFrame(report).transpose()\n","    df_report.to_csv(report_file, sep='\\t')\n"],"metadata":{"id":"yoG9PjQkzt5k"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#train\n","def train_and_predict(df1, df2,\n","                      labels_dict, num_labels, batchsize, epochs, lr,\n","                      model_save_dir,\n","                      all_preds_file,\n","                      report_file,\n","                      reverse_dict, run):\n","\n","  finetuned_model = train(df1, df2,\n","                        labels_dict, num_labels, batchsize, epochs, lr,\n","                        model_save_dir=model_save_dir+str(run))\n","\n","  #predict with each classifier on the three gold datasets\n","  golds, preds = predict(finetuned_model, eval_hate, 8, '1', labels_dict[1], use_gold_labels=True) #main task\n","  auxpreds = predict(finetuned_model, eval_hate, 8, '2', labels_dict[1]) #aux task\n","\n","  #save classification reports\n","  report(golds, preds, report_file=report_file+str(run)+'.tsv')\n","\n","  #save all predictions on gold datasets\n","  with open(all_preds_file+str(run)+'.tsv',\"w\") as f:\n","      f.write(\"Gold\\tmaintask\\tauxtask\\n\")\n","      for gold, pred, auxpred in zip(golds, preds, auxpreds):\n","        f.write(reverse_dict[1][gold]+\"\\t\"+reverse_dict[1][pred]+\"\\t\"+reverse_dict[2][auxpred]+\"\\n\")"],"metadata":{"id":"ULMPYp8qIFlw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### AbuseEval + sentiment"],"metadata":{"id":"sul9F7CJhc1v"}},{"cell_type":"code","source":["print('----Loading train data----')\n","train_hate1 = pd.read_csv(f'{dir}data/abuseeval/train.tsv', sep='\\t')\n","train_hate2 = pd.read_csv(f'{dir}data/abuseeval/dev.tsv', sep='\\t')\n","train_hate = pd.concat([train_hate1, train_hate2])\n","\n","train_sent = pd.read_csv(f'{dir}data/sentiment/test2016.tsv', sep='\\t')\n","\n","print('----Loading test data----')\n","eval_hate = pd.read_csv(f'{dir}data/abuseeval/test.tsv', sep='\\t')"],"metadata":{"id":"wORUUwjeAjur"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_dict = {1: {0:0, 1:1, 2:2},\n","               2: {'neutral': 0, 'positive':1, 'negative': 2}}\n","\n","reverse_labels_dict ={1: {0:'not_abuse', 1:'explicit_abuse', 2:'implicit_abuse'},\n","                      2: {0: 'neutral', 1: 'positive', 2: 'negative'}}\n","\n","num_labels = [3, 3]"],"metadata":{"id":"mMLNY2eajBjY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for run in [1, 2, 3, 4, 5]:\n","  train_and_predict(train_hate, train_sent,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/abuseeval+sent_3ep_16_2e-5_RUN',\n","                  dir+'results/abuseeval+sent_abuse_RUN',\n","                  dir+'results/abuseeval+sent_report',\n","                  reverse_labels_dict, run)"],"metadata":{"id":"CdZHfSmkk5SJ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### AbuseEval + emotion"],"metadata":{"id":"E2sD6Wa5iUWD"}},{"cell_type":"code","source":["print('----Loading train data----')\n","train_hate1 = pd.read_csv(f'{dir}data/abuseeval/train.tsv', sep='\\t')\n","train_hate2 = pd.read_csv(f'{dir}data/abuseeval/dev.tsv', sep='\\t')\n","train_hate = pd.concat([train_hate1, train_hate2])\n","train_emo = pd.read_csv(f'{dir}data/emotion/tec.tsv', sep='\\t')\n","print('----Loading test data----')\n","eval_hate = pd.read_csv(f'{dir}data/abuseeval/test.tsv', sep='\\t')"],"metadata":{"id":"XzQoJ6PaiXfk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_dict = {1: {0:0, 1:1, 2:2},\n","               2: {'anger': 0, 'disgust':1, 'fear': 2, 'joy': 3, 'sadness': 4, 'surprise': 5}}\n","\n","reverse_labels_dict ={1: {0:'not_abuse', 1:'explicit_abuse', 2:'implicit_abuse'},\n","                      2: {0: 'anger', 1: 'disgust', 2: 'fear', 3: 'joy', 4: 'sadness', 5: 'surprise'}}\n","\n","num_labels = [3, 6]"],"metadata":{"id":"4F9brgF4jC44"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for run in [1, 2, 3, 4, 5]:\n","  \ttrain_and_predict(train_hate, train_emo,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/abuseeval+emo_3ep_16_2e-5_RUN',\n","                  dir+'results/abuseeval+emo_RUN',\n","                  dir+'results/abuseeval+emo_report',\n","                  reverse_labels_dict, run)"],"metadata":{"id":"E2qqWFMGlV2Z"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### AbuseEval + sarcasm"],"metadata":{"id":"d3ZcD7xMiwUQ"}},{"cell_type":"code","source":["print('----Loading train data----')\n","train_hate1 = pd.read_csv(f'{dir}data/abuseeval/train.tsv', sep='\\t')\n","train_hate2 = pd.read_csv(f'{dir}data/abuseeval/dev.tsv', sep='\\t')\n","train_hate = pd.concat([train_hate1, train_hate2])\n","\n","train_sarc1 = pd.read_csv(f'{dir}data/sarcasm/twitter-train-nocontext.tsv', sep='\\t')\n","train_sarc2 = pd.read_csv(f'{dir}data/sarcasm/reddit-train-nocontext.tsv', sep='\\t')\n","train_sarcasm = pd.concat([train_sarc1, train_sarc2])\n","\n","print('----Loading test data----')\n","eval_hate = pd.read_csv(f'{dir}data/abuseeval/test.tsv', sep='\\t')"],"metadata":{"id":"2vFdLckfilKb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_dict = {1: {0:0, 1:1, 2:2},\n","               2: {'NOT_SARCASM': 0, 'SARCASM':1}}\n","\n","reverse_labels_dict = {1: {0:'not_abuse', 1:'explicit_abuse', 2:'implicit_abuse'},\n","               2: {0: 'not_sarcasm', 1: 'sarcasm'}}\n","\n","num_labels = [3, 2]"],"metadata":{"id":"8nFgWAQCjDUl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for run in [1, 2, 3, 4, 5]:\n","  train_and_predict(train_hate, train_sarcasm,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/abuseeval+sarc_3ep_16_2e-5_RUN',\n","                  dir+'results/abuseeval+sarc_RUN',\n","                  dir+'results/abuseeval+sarc_report',\n","                  reverse_labels_dict, run)"],"metadata":{"id":"8lGcghAflhcL"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### AbuseEval + irony"],"metadata":{"id":"EFTg9tmbiym_"}},{"cell_type":"code","source":["print('----Loading train data----')\n","train_hate1 = pd.read_csv(f'{dir}data/abuseeval/train.tsv', sep='\\t')\n","train_hate2 = pd.read_csv(f'{dir}data/abuseeval/dev.tsv', sep='\\t')\n","train_hate = pd.concat([train_hate1, train_hate2])\n","\n","train_irony = pd.read_csv(f'{dir}data/irony/train.tsv', sep='\\t')\n","\n","print('----Loading test data----')\n","eval_hate = pd.read_csv(f'{dir}data/abuseeval/test.tsv', sep='\\t')"],"metadata":{"id":"EI_Kw1OsirBw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_dict = {1: {0:0, 1:1, 2:2},\n","               2: {0:0, 1:1}}\n","\n","reverse_labels_dict = {1: {0:'not_abuse', 1:'explicit_abuse', 2:'implicit_abuse'},\n","               2: {0:'not_irony', 1:'irony'}}\n","\n","num_labels = [3, 2]"],"metadata":{"id":"bsY_xWmQjD44"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for run in [1, 2, 3, 4, 5]:\n","  train_and_predict(train_hate, train_irony,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/abuseeval+iro_3ep_16_2e-5_RUN',\n","                  dir+'results/abuseeval+iro_RUN',\n","                  dir+'results/abuseeval+iro_report',\n","                  reverse_labels_dict, run)"],"metadata":{"id":"-qwtXUuPlk36"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### TRAC + sentiment"],"metadata":{"id":"KhsF_Z2AAB12"}},{"cell_type":"code","source":["train_hate1 = pd.read_csv(f'{dir}data/trac/train.tsv', sep='\\t')\n","train_hate2 = pd.read_csv(f'{dir}data/trac/dev.tsv', sep='\\t')\n","train_hate = pd.concat([train_hate1, train_hate2])\n","\n","eval_hate = pd.read_csv(f'{dir}data/trac/fb_test.tsv', sep='\\t')\n","\n","train_sent = pd.read_csv(f'{dir}data/sentiment/test2016.tsv', sep='\\t')"],"metadata":{"id":"Y2iZ0wr7llPB"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_dict = {1: {'NAG':0, 'OAG':1, 'CAG':2},\n","               2: {'neutral': 0, 'positive':1, 'negative': 2}}\n","\n","reverse_labels_dict ={1: {0: 'not_aggression', 1: 'overt_aggression', 2: 'covert_aggression'},\n","                      2: {0: 'neutral', 1: 'positive', 2: 'negative'}}\n","\n","num_labels = [3, 3]"],"metadata":{"id":"9le_HtrQCLQP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for run in [1, 2, 3, 4, 5]:\n","  train_and_predict(train_hate, train_sent,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/trac+sent_3ep_16_2e-5_RUN',\n","                  dir+'results/trac+sent_RUN',\n","                  dir+'results/trac+sent_report',\n","                  reverse_labels_dict, run)"],"metadata":{"id":"fF3nTZhOCbYG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### TRAC + emotion"],"metadata":{"id":"wRcpiRDBAJPd"}},{"cell_type":"code","source":["train_hate1 = pd.read_csv(f'{dir}data/trac/train.tsv', sep='\\t')\n","train_hate2 = pd.read_csv(f'{dir}data/trac/dev.tsv', sep='\\t')\n","train_hate = pd.concat([train_hate1, train_hate2])\n","\n","eval_hate = pd.read_csv(f'{dir}data/trac/fb_test.tsv', sep='\\t')\n","\n","train_emo = pd.read_csv(f'{dir}data/emotion/tec.tsv', sep='\\t')"],"metadata":{"id":"hTuXlFphBZDp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_dict = {1: {'NAG':0, 'OAG':1, 'CAG':2},\n","               2: {'anger': 0, 'disgust':1, 'fear': 2, 'joy': 3, 'sadness': 4, 'surprise': 5}}\n","\n","reverse_labels_dict = {1: {0: 'not_aggression', 1: 'overt_aggression', 2: 'covert_aggression'},\n","               2: {0: 'anger', 1: 'disgust', 2: 'fear', 3: 'joy', 4: 'sadness', 5: 'surprise'}}\n","\n","num_labels = [3, 6]"],"metadata":{"id":"uJnBp-VzBZGP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for run in [1, 2, 3, 4, 5]:\n","  train_and_predict(train_hate, train_emo,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/trac+emo_3ep_16_2e-5_RUN',\n","                  dir+'results/trac+emo_RUN',\n","                  dir+'results/trac+emo_report',\n","                  reverse_labels_dict, run)"],"metadata":{"id":"OpSVpKK0Bcev"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### TRAC + sarcasm"],"metadata":{"id":"PDxV4qoJAJSH"}},{"cell_type":"code","source":["train_hate1 = pd.read_csv(f'{dir}data/trac/train.tsv', sep='\\t')\n","train_hate2 = pd.read_csv(f'{dir}data/trac/dev.tsv', sep='\\t')\n","train_hate = pd.concat([train_hate1, train_hate2])\n","\n","eval_hate = pd.read_csv(f'{dir}data/trac/fb_test.tsv', sep='\\t')\n","\n","train_sarc1 = pd.read_csv(f'{dir}data/sarcasm/twitter-train-nocontext.tsv', sep='\\t')\n","train_sarc2 = pd.read_csv(f'{dir}data/sarcasm/reddit-train-nocontext.tsv', sep='\\t')\n","train_sarcasm = pd.concat([train_sarc1, train_sarc2])"],"metadata":{"id":"2joG0524AJse"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_dict = {1: {'NAG':0, 'OAG':1, 'CAG':2},\n","               2: {'NOT_SARCASM': 0, 'SARCASM':1}}\n","\n","reverse_labels_dict = {1: {0: 'not_aggression', 1: 'overt_aggression', 2: 'covert_aggression'},\n","                       2: {0: 'not_sarcasm', 1: 'sarcasm'}}\n","\n","num_labels = [3, 2]"],"metadata":{"id":"7_72OvJ9AJul"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for run in [1, 2, 3, 4, 5]:\n","  train_and_predict(train_hate, train_sarcasm,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/trac+sarc_3ep_16_2e-5_RUN',\n","                  dir+'results/trac+sarc_RUN',\n","                  dir+'results/trac+sarc_report',\n","                  reverse_labels_dict, run)"],"metadata":{"id":"c5OIuId5ifYk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### TRAC + irony"],"metadata":{"id":"OSRoebEwm5iN"}},{"cell_type":"code","source":["train_hate1 = pd.read_csv(f'{dir}data/trac/train.tsv', sep='\\t')\n","train_hate2 = pd.read_csv(f'{dir}data/trac/dev.tsv', sep='\\t')\n","train_hate = pd.concat([train_hate1, train_hate2])\n","\n","train_irony = pd.read_csv(f'{dir}data/irony/train.tsv', sep='\\t')\n","\n","eval_hate = pd.read_csv(f'{dir}data/trac/fb_test.tsv', sep='\\t')"],"metadata":{"id":"vefmqTKaiDLS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_dict = {1: {'NAG':0, 'OAG':1, 'CAG':2},\n","               2: {0:0, 1:1}}\n","\n","reverse_labels_dict = {1: {0: 'not_aggression', 1: 'overt_aggression', 2: 'covert_aggression'},\n","               2: {0:'not_irony', 1:'irony'}}\n","\n","num_labels = [3, 2]"],"metadata":{"id":"YwNmhV0mjIfJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for run in [1, 2, 3, 4, 5]:\n","  train_and_predict(train_hate, train_irony,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/trac+iro_3ep_16_2e-5_RUN',\n","                  dir+'results/trac+iro_RUN',\n","                  dir+'results/trac+iro_report',\n","                  reverse_labels_dict, run)"],"metadata":{"id":"fGWVtpjf_Vj5"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### IHC + sentiment"],"metadata":{"id":"-THhbjEam8aM"}},{"cell_type":"code","source":["train_hate = pd.read_csv(f'{dir}data/implicithate/train1.tsv', sep='\\t')\n","\n","eval_hate = pd.read_csv(f'{dir}data/implicithate/test1.tsv', sep='\\t')\n","\n","train_sent = pd.read_csv(f'{dir}data/sentiment/test2016.tsv', sep='\\t')"],"metadata":{"id":"lUuwaaBiiDOI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_dict = {1: {'not_hate':0, 'explicit_hate':1, 'implicit_hate':2},\n","               2: {'neutral': 0, 'positive':1, 'negative': 2}}\n","\n","reverse_labels_dict ={1: {0: 'not_hate', 1: 'explicit_hate', 2: 'implicit_hate'},\n","                      2: {0: 'neutral', 1: 'positive', 2: 'negative'}}\n","\n","num_labels = [3, 3]"],"metadata":{"id":"FAHXTUYfjK35"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for run in [1, 2, 3, 4, 5]:\n","  train_and_predict(train_hate, train_sent,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/ihc+sent_3ep_16_2e-5_RUN',\n","                  dir+'results/ihc+sent_RUN',\n","                  dir+'results/ihc+sent_report',\n","                  reverse_labels_dict, run)"],"metadata":{"id":"MIugUOwbFbAc"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### IHC + emotion"],"metadata":{"id":"y3UsZT9FC4jX"}},{"cell_type":"code","source":["train_hate = pd.read_csv(f'{dir}data/implicithate/train1.tsv', sep='\\t')\n","train_emo = pd.read_csv(f'{dir}data/emotion/tec.tsv', sep='\\t')\n","\n","eval_hate = pd.read_csv(f'{dir}data/implicithate/test1.tsv', sep='\\t')"],"metadata":{"id":"svJrPT-6DhA6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_dict = {1: {'not_hate':0, 'explicit_hate':1, 'implicit_hate':2},\n","               2: {'anger': 0, 'disgust':1, 'fear': 2, 'joy': 3, 'sadness': 4, 'surprise': 5}}\n","\n","reverse_labels_dict ={1: {0: 'not_hate', 1: 'explicit_hate', 2: 'implicit_hate'},\n","                      2: {0: 'anger', 1: 'disgust', 2: 'fear', 3: 'joy', 4: 'sadness', 5: 'surprise'}}\n","\n","num_labels = [3, 6]"],"metadata":{"id":"MOe0XwxsDhDY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for run in [1, 2, 3, 4, 5]:\n","  train_and_predict(train_hate, train_emo,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/ihc+emo_3ep_16_2e-5_RUN',\n","                  dir+'results/ihc+emo_RUN',\n","                  dir+'results/ihc+emo_report',\n","                  reverse_labels_dict, run)"],"metadata":{"id":"1p-LXe7EEAuG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### IHC + sarcasm"],"metadata":{"id":"V7KxtFpRC4rF"}},{"cell_type":"code","source":["train_hate = pd.read_csv(f'{dir}data/implicithate/train1.tsv', sep='\\t')\n","\n","eval_hate = pd.read_csv(f'{dir}data/implicithate/test1.tsv', sep='\\t')\n","\n","train_sarc1 = pd.read_csv(f'{dir}data/sarcasm/twitter-train-nocontext.tsv', sep='\\t')\n","train_sarc2 = pd.read_csv(f'{dir}data/sarcasm/reddit-train-nocontext.tsv', sep='\\t')\n","train_sarcasm = pd.concat([train_sarc1, train_sarc2])"],"metadata":{"id":"nurzWnT8DkG3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_dict = {1: {'not_hate':0, 'explicit_hate':1, 'implicit_hate':2},\n","               2: {'NOT_SARCASM': 0, 'SARCASM':1}}\n","\n","reverse_labels_dict ={1: {0: 'not_hate', 1: 'explicit_hate', 2: 'implicit_hate'},\n","                      2: {0: 'not_sarcasm', 1: 'sarcasm'}}\n","\n","num_labels = [3, 2]"],"metadata":{"id":"zuHaiCFuDkLn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for run in [1, 2, 3, 4, 5]:\n","  train_and_predict(train_hate, train_sarcasm,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/ihc+sarc_3ep_16_2e-5_RUN',\n","                  dir+'results/ihc+sarc_RUN',\n","                  dir+'results/ihc+sarc_report',\n","                  reverse_labels_dict, run)"],"metadata":{"id":"78cPdBc6Egsn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### IHC + irony"],"metadata":{"id":"edi4Jek_C4zG"}},{"cell_type":"code","source":["train_hate = pd.read_csv(f'{dir}data/implicithate/train1.tsv', sep='\\t')\n","\n","eval_hate = pd.read_csv(f'{dir}data/implicithate/test1.tsv', sep='\\t')\n","\n","train_irony = pd.read_csv(f'{dir}data/irony/train.tsv', sep='\\t')"],"metadata":{"id":"GH1Kr6vFDmlh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_dict = {1: {'not_hate':0, 'explicit_hate':1, 'implicit_hate':2},\n","               2: {0:0, 1:1}}\n","\n","reverse_labels_dict ={1: {0: 'not_hate', 1: 'explicit_hate', 2: 'implicit_hate'},\n","                      2: {0:'not_irony', 1:'irony'}}\n","\n","num_labels = [3, 2]"],"metadata":{"id":"lknrhKxzDmoe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for run in [1, 2, 3, 4, 5]:\n","  train_and_predict(train_hate, train_irony,\n","                  labels_dict, num_labels, 16, 3, 2e-5,\n","                  dir+'models/ihc+iro_3ep_16_2e-5_RUN',\n","                  dir+'results/ihc+iro_RUN',\n","                  dir+'results/ihc+iro_report',\n","                  reverse_labels_dict, run)"],"metadata":{"id":"VTVagwvIFBbI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"RXYfUN35hpVZ"},"execution_count":null,"outputs":[]}]}